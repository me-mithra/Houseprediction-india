# -*- coding: utf-8 -*-
"""houseprediction_india.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_2pP_CqHxU5hi8d3etkaNTEeY9ctZHgK
"""

# üì¶ Install libraries (if not already installed)
!pip install matplotlib seaborn scikit-learn

# üßæ Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# A. üì• Data Acquisition
df = pd.read_csv('Real_Estate_Data.csv')  # Path to uploaded dataset

def clean_price(val):
    val = str(val).replace('‚Çπ', '').replace(',', '').strip().lower()

    # Handle Crore
    if 'cr' in val:
        try:
            return float(val.replace('cr', '').strip()) * 100  # Convert to Lakhs
        except:
            return np.nan

    # Handle Lakh
    elif 'l' in val:
        try:
            return float(val.replace('l', '').strip())
        except:
            return np.nan

    # Ignore unusual formats like "acs", "sqft", etc.
    else:
        return np.nan

df['Cleaned_Price(Lakhs)'] = df['Price'].apply(clean_price)
df.dropna(subset=['Cleaned_Price(Lakhs)'], inplace=True)
df.drop(['Price'], axis=1, inplace=True)


# Handle missing values
df.fillna(method='ffill', inplace=True)

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import time



# C. üìä Exploratory Data Analysis
print(df.info())
print(df.describe())

# Price Distribution
plt.figure(figsize=(8,4))
sns.histplot(df['Cleaned_Price(Lakhs)'], bins=50, kde=True)
plt.title('Distribution of Prices (in Lakhs)')
plt.show()

# D. ‚öôÔ∏è Feature Engineering
# Encode categorical variables
df_encoded = pd.get_dummies(df, drop_first=True)

# E. üèóÔ∏è Model Building
X = df_encoded.drop('Cleaned_Price(Lakhs)', axis=1)
y = df_encoded['Cleaned_Price(Lakhs)']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# F. üìà Model Evaluation
y_pred = model.predict(X_test)
print("R2 Score:", r2_score(y_test, y_pred))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred)))

# G. üìâ Visualization & Insights
plt.figure(figsize=(6,6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.xlabel("Actual Price (Lakhs)")
plt.ylabel("Predicted Price (Lakhs)")
plt.title("Actual vs Predicted Prices")
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')  # Diagonal line
plt.show()

# Feature Importance
importances = model.feature_importances_
feat_names = X.columns
imp_df = pd.DataFrame({'Feature': feat_names, 'Importance': importances})
imp_df.sort_values(by='Importance', ascending=False, inplace=True)

plt.figure(figsize=(10,6))
sns.barplot(x='Importance', y='Feature', data=imp_df.head(10))
plt.title('Top 10 Important Features')
plt.show()

# H. üöÄ Deployment Placeholder
# For now, we just simulate saving the model
import joblib
joblib.dump(model, 'house_price_model.pkl')
print("Model saved for deployment.")